# ğŸ“ˆ Linear Regression

[![Algorithm](https://img.shields.io/badge/Algorithm-Linear%20Regression-blue.svg)](https://en.wikipedia.org/wiki/Linear_regression)
[![Type](https://img.shields.io/badge/Type-Regression-green.svg)](https://en.wikipedia.org/wiki/Regression_analysis)
[![Complexity](https://img.shields.io/badge/Time%20Complexity-O(n)-orange.svg)](https://en.wikipedia.org/wiki/Time_complexity)

## ğŸ¯ Overview

Linear Regression is a **fundamental statistical method** that models the relationship between a dependent variable and independent variables by fitting a linear equation to observed data. It's the foundation of many machine learning algorithms and provides interpretable results with clear mathematical relationships.

## ğŸ§  Algorithm Workflow

```mermaid
flowchart TD
    A[ğŸ“Š Training Dataset] --> B[ğŸ“ Initialize Parameters]
    B --> C[ğŸ“ˆ Calculate Predictions]
    C --> D[ğŸ“Š Compute Cost Function]
    D --> E[ğŸ”„ Update Parameters]
    E --> F{ğŸ¯ Convergence?}
    F -->|No| C
    F -->|Yes| G[âœ… Optimal Model]
    
    H[ğŸ” New Data Point] --> I[ğŸ“ Apply Linear Equation]
    I --> J[ğŸ“ˆ Calculate Prediction]
    J --> K[ğŸ“Š Return Result]
    
    G --> I
    
    style A fill:#e1f5fe
    style H fill:#e8f5e8
    style K fill:#fff3e0
    style G fill:#f3e5f5
```

## ğŸ“ Mathematical Foundation

### Simple Linear Regression
```
y = Î²â‚€ + Î²â‚x + Îµ
```

### Multiple Linear Regression
```
y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™ + Îµ
```

### Matrix Form
```
y = XÎ² + Îµ
```

Where:
- **y**: Dependent variable (target)
- **X**: Independent variables (features)
- **Î²**: Coefficients (parameters)
- **Îµ**: Error term (residuals)

### Cost Function (Mean Squared Error)
```
MSE = (1/2m) Ã— Î£(hÎ¸(xâ½â±â¾) - yâ½â±â¾)Â²
```

### Normal Equation
```
Î² = (Xáµ€X)â»Â¹Xáµ€y
```

## ğŸ” Types of Linear Regression

```mermaid
graph TB
    A[ğŸ“ˆ Linear Regression] --> B[ğŸ“Š Simple Linear Regression]
    A --> C[ğŸ“ˆ Multiple Linear Regression]
    A --> D[ğŸ”§ Regularized Regression]
    
    B --> B1[ğŸ“‰ One Feature]
    B --> B2[ğŸ“ˆ Straight Line Fit]
    
    C --> C1[ğŸ“Š Multiple Features]
    C --> C2[ğŸ¯ Hyperplane Fit]
    
    D --> D1[ğŸ¯ Ridge Regression - L2]
    D --> D2[ğŸ“Š Lasso Regression - L1]
    D --> D3[âš–ï¸ Elastic Net]
    
    style A fill:#ff9999
    style B fill:#99ccff
    style C fill:#99ff99
    style D fill:#ffcc99
```

## ğŸ¯ Regression Variants

### Ridge Regression (L2 Regularization)
```
Cost = MSE + Î± Ã— Î£Î²áµ¢Â²
```

### Lasso Regression (L1 Regularization)
```
Cost = MSE + Î± Ã— Î£|Î²áµ¢|
```

### Elastic Net
```
Cost = MSE + Î±â‚ Ã— Î£|Î²áµ¢| + Î±â‚‚ Ã— Î£Î²áµ¢Â²
```

## âœ… Assumptions

```mermaid
graph LR
    A[Linear Regression Assumptions] --> B[ğŸ“ˆ Linearity]
    A --> C[ğŸ”„ Independence]
    A --> D[ğŸ“Š Homoscedasticity]
    A --> E[ğŸ“ˆ Normality]
    A --> F[âŒ No Multicollinearity]
    
    B --> B1[Linear relationship between X and y]
    C --> C1[Observations are independent]
    D --> D1[Constant variance of residuals]
    E --> E1[Residuals are normally distributed]
    F --> F1[Features are not highly correlated]
    
    style A fill:#ff9999
    style B fill:#99ccff
    style C fill:#99ff99
    style D fill:#ffcc99
    style E fill:#cc99ff
    style F fill:#ffff99
```

## âœ… Advantages

- **ğŸ” Interpretability**: Clear understanding of feature impact
- **âš¡ Fast Training**: Closed-form solution available
- **ğŸ“Š Statistical Inference**: Confidence intervals and p-values
- **ğŸ¯ Baseline Model**: Good starting point for regression problems
- **ğŸ“ˆ Linear Relationships**: Excellent for linear patterns
- **ğŸ’¾ Memory Efficient**: Low computational requirements
- **ğŸ”§ No Hyperparameters**: Simple implementation

## âŒ Disadvantages

- **ğŸ“ˆ Linear Assumption**: Cannot capture non-linear relationships
- **ğŸ¯ Outlier Sensitive**: Heavily influenced by outliers
- **ğŸ“Š Feature Scaling**: Requires normalized features
- **ğŸ”„ Multicollinearity**: Problems with correlated features
- **ğŸ“‰ Underfitting**: May be too simple for complex data
- **ğŸª Limited Flexibility**: Cannot model interactions without feature engineering

## ğŸ¯ Use Cases & Applications

| Domain | Application | Example |
|--------|-------------|----------|
| **ğŸ  Real Estate** | Price Prediction | House price based on size, location |
| **ğŸ’° Finance** | Risk Assessment | Stock price prediction |
| **ğŸ“Š Marketing** | Sales Forecasting | Revenue prediction from advertising spend |
| **ğŸ¥ Healthcare** | Medical Analysis | Drug dosage optimization |
| **ğŸŒ¾ Agriculture** | Crop Yield** | Yield prediction from weather data |
| **ğŸš— Automotive** | Fuel Efficiency | MPG prediction from car features |

## ğŸ“ Project Structure

```
Linear Regression/
â”œâ”€â”€ ğŸ““ Simple+Linear+Regression.ipynb                          # Single feature regression
â”œâ”€â”€ ğŸ““ Multiple+Linear+Regression-+Economics+Dataset.ipynb     # Multiple feature regression
â”œâ”€â”€ ğŸ““ Model Training.ipynb                                     # Complete training process
â”œâ”€â”€ ğŸ““ Ridge, Lasso Regression.ipynb                          # Regularization techniques
â”œâ”€â”€ ğŸ“Š Algerian_forest_fires_cleaned_dataset.csv              # Cleaned dataset
â”œâ”€â”€ ğŸ“Š Algerian_forest_fires_dataset_UPDATE.csv               # Updated dataset
â”œâ”€â”€ ğŸ“Š economic_index.csv                                      # Economics data
â”œâ”€â”€ ğŸ“Š height-weight.csv                                       # Simple regression data
â””â”€â”€ ğŸ“„ README.md                                               # This documentation
```

## ğŸš€ Implementation Guide

### 1. Simple Linear Regression
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load and prepare data
data = pd.read_csv('height-weight.csv')
X = data[['Height']].values
y = data['Weight'].values

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.4f}")
print(f"RÂ² Score: {r2:.4f}")
print(f"Coefficient: {model.coef_[0]:.4f}")
print(f"Intercept: {model.intercept_:.4f}")
```

### 2. Multiple Linear Regression
```python
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_regression

# Load economics dataset
data = pd.read_csv('economic_index.csv')
X = data.drop('target', axis=1)
y = data['target']

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Feature selection
selector = SelectKBest(score_func=f_regression, k=5)
X_selected = selector.fit_transform(X_scaled, y)

# Split and train
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = model.predict(X_test)
print(f"Multiple RÂ² Score: {r2_score(y_test, y_pred):.4f}")
```

### 3. Ridge Regression (L2 Regularization)
```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

# Ridge regression with cross-validation
ridge = Ridge()
param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0, 1000.0]}

grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

best_ridge = grid_search.best_estimator_
y_pred_ridge = best_ridge.predict(X_test)

print(f"Best Alpha: {grid_search.best_params_['alpha']}")
print(f"Ridge RÂ² Score: {r2_score(y_test, y_pred_ridge):.4f}")
```

### 4. Lasso Regression (L1 Regularization)
```python
from sklearn.linear_model import Lasso

# Lasso regression for feature selection
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

y_pred_lasso = lasso.predict(X_test)

# Check which features were selected (non-zero coefficients)
selected_features = X.columns[lasso.coef_ != 0]
print(f"Selected features: {list(selected_features)}")
print(f"Lasso RÂ² Score: {r2_score(y_test, y_pred_lasso):.4f}")
```

### 5. Elastic Net
```python
from sklearn.linear_model import ElasticNet

# Elastic Net combines L1 and L2 regularization
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_net.fit(X_train, y_train)

y_pred_elastic = elastic_net.predict(X_test)
print(f"Elastic Net RÂ² Score: {r2_score(y_test, y_pred_elastic):.4f}")
```

## ğŸ“Š Model Evaluation & Diagnostics

### Residual Analysis
```python
# Calculate residuals
residuals = y_test - y_pred

# Plot residuals
plt.figure(figsize=(12, 4))

# Residuals vs Fitted
plt.subplot(1, 3, 1)
plt.scatter(y_pred, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs Fitted')

# Q-Q plot for normality
from scipy import stats
plt.subplot(1, 3, 2)
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('Q-Q Plot')

# Histogram of residuals
plt.subplot(1, 3, 3)
plt.hist(residuals, bins=20, edgecolor='black')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Residual Distribution')

plt.tight_layout()
plt.show()
```

### Statistical Tests
```python
from scipy.stats import jarque_bera, durbin_watson
from statsmodels.stats.diagnostic import het_breuschpagan

# Test for normality of residuals
jb_stat, jb_pvalue = jarque_bera(residuals)
print(f"Jarque-Bera test p-value: {jb_pvalue:.4f}")

# Test for autocorrelation
dw_stat = durbin_watson(residuals)
print(f"Durbin-Watson statistic: {dw_stat:.4f}")

# Test for heteroscedasticity
# Note: This requires fitted model from statsmodels
```

## ğŸ”§ Feature Engineering

### Polynomial Features
```python
from sklearn.preprocessing import PolynomialFeatures

# Create polynomial features
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)

# Train polynomial regression
X_train_poly, X_test_poly, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

poly_model = LinearRegression()
poly_model.fit(X_train_poly, y_train)

y_pred_poly = poly_model.predict(X_test_poly)
print(f"Polynomial RÂ² Score: {r2_score(y_test, y_pred_poly):.4f}")
```

### Interaction Terms
```python
# Create interaction features
from itertools import combinations

# Manual interaction terms
X_interactions = X.copy()
for col1, col2 in combinations(X.columns, 2):
    X_interactions[f"{col1}_x_{col2}"] = X[col1] * X[col2]
```

## ğŸ“ˆ Model Comparison

```python
# Compare different regression models
models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1),
    'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    results[name] = {
        'RÂ²': r2_score(y_test, y_pred),
        'MSE': mean_squared_error(y_test, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))
    }

# Display results
results_df = pd.DataFrame(results).T
print(results_df.round(4))
```

## ğŸ“š Learning Resources

- **Simple Regression**: [`Simple+Linear+Regression.ipynb`](./Simple+Linear+Regression.ipynb)
- **Multiple Regression**: [`Multiple+Linear+Regression-+Economics+Dataset.ipynb`](./Multiple+Linear+Regression-+Economics+Dataset.ipynb)
- **Model Training**: [`Model Training.ipynb`](./Model%20Training.ipynb)
- **Regularization**: [`Ridge, Lasso Regression.ipynb`](./Ridge,%20Lasso%20Regression.ipynb)
- **Scikit-learn Documentation**: [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)
- **Theory**: [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)

## ğŸ“ Key Takeaways

1. **ğŸ“Š Foundation**: Understanding linear regression is crucial for all ML
2. **ğŸ” Interpretability**: Coefficients show feature importance and direction
3. **âš–ï¸ Regularization**: Use Ridge/Lasso to prevent overfitting
4. **ğŸ“ˆ Assumptions**: Verify linearity, independence, and normality
5. **ğŸ¯ Feature Engineering**: Polynomial and interaction terms for non-linearity
6. **ğŸ“Š Diagnostics**: Always check residuals and model assumptions

---

*Navigate back to [Main Repository](../README.md) | Previous: [Decision Tree](../Decision%20Tree/README.md) | Next: [KNN](../KNN/README.md)*
