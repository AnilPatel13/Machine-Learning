# ğŸš€ Gradient Boosting

[![Algorithm](https://img.shields.io/badge/Algorithm-Gradient%20Boosting-brightgreen.svg)](https://en.wikipedia.org/wiki/Gradient_boosting)
[![Type](https://img.shields.io/badge/Type-Ensemble%20Method-blue.svg)](https://en.wikipedia.org/wiki/Ensemble_learning)
[![Complexity](https://img.shields.io/badge/Time%20Complexity-O(n%20log%20n)-orange.svg)](https://en.wikipedia.org/wiki/Time_complexity)

## ğŸ¯ Overview

Gradient Boosting is a **powerful ensemble learning technique** that builds models sequentially, where each new model corrects the errors made by previous models. It combines weak learners (typically decision trees) to create a strong predictor by optimizing a differentiable loss function using gradient descent in function space.

## ğŸ§  Algorithm Workflow

```mermaid
flowchart TD
    A[ğŸ“Š Training Dataset] --> B[ğŸ¯ Initialize with Base Prediction]
    B --> C[ğŸ“ˆ Calculate Residuals/Gradients]
    C --> D[ğŸŒ³ Train Weak Learner on Residuals]
    D --> E[ğŸ¯ Find Optimal Step Size]
    E --> F[ğŸ“Š Update Model Prediction]
    F --> G{ğŸ”„ Max Iterations Reached?}
    G -->|No| H[ğŸ“ˆ Calculate New Residuals]
    H --> C
    G -->|Yes| I[âœ… Final Ensemble Model]
    
    J[ğŸ” New Data Point] --> K[ğŸŒ³ Apply All Weak Learners]
    K --> L[ğŸ“Š Sum Weighted Predictions]
    L --> M[ğŸ“ˆ Final Prediction]
    
    I --> K
    
    style A fill:#e1f5fe
    style J fill:#e8f5e8
    style M fill:#fff3e0
    style I fill:#f3e5f5
```

## ğŸ“ Mathematical Foundation

### General Gradient Boosting Algorithm
```
Fâ‚€(x) = argmin_Î³ Î£áµ¢ L(yáµ¢, Î³)
```

### For iteration m = 1 to M:
```
1. Compute pseudo-residuals:
   ráµ¢â‚˜ = -[âˆ‚L(yáµ¢, F(xáµ¢))/âˆ‚F(xáµ¢)]_{F=Fâ‚˜â‚‹â‚}

2. Fit weak learner hâ‚˜(x) to pseudo-residuals

3. Find optimal step size:
   Î³â‚˜ = argmin_Î³ Î£áµ¢ L(yáµ¢, Fâ‚˜â‚‹â‚(xáµ¢) + Î³hâ‚˜(xáµ¢))

4. Update model:
   Fâ‚˜(x) = Fâ‚˜â‚‹â‚(x) + Î³â‚˜hâ‚˜(x)
```

### Common Loss Functions

#### Regression (Squared Loss)
```
L(y, F(x)) = Â½(y - F(x))Â²
Gradient: ráµ¢ = yáµ¢ - F(xáµ¢)
```

#### Classification (Logistic Loss)
```
L(y, F(x)) = log(1 + exp(-yF(x)))
Gradient: ráµ¢ = yáµ¢/(1 + exp(yáµ¢F(xáµ¢)))
```

## âœ… Advantages

- **ğŸ¯ High Accuracy**: Often achieves excellent predictive performance
- **ğŸ”„ Flexible**: Works with various loss functions
- **ğŸ“Š Handles Mixed Data**: Works with numerical and categorical features
- **ğŸŒ³ Feature Importance**: Provides feature importance measures
- **ğŸª Robust**: Less prone to overfitting than individual trees
- **ğŸ“ˆ Interpretable**: Tree-based models are relatively interpretable

## âŒ Disadvantages

- **â° Sequential Training**: Cannot be easily parallelized
- **ğŸ”§ Hyperparameter Sensitive**: Many parameters to tune
- **ğŸ’¾ Memory Usage**: Can be memory intensive
- **âš¡ Overfitting Risk**: Can overfit with too many iterations
- **ğŸ¯ Sensitive to Outliers**: Affected by outliers in data

## ğŸ¯ Use Cases & Applications

| Domain | Application | Example |
|--------|-------------|----------|
| **ğŸ’° Finance** | Risk Modeling | Credit scoring, fraud detection |
| **ğŸ›’ E-commerce** | Recommendation | Product recommendations, ranking |
| **ğŸ¥ Healthcare** | Medical Prediction | Disease diagnosis, treatment outcomes |
| **ğŸ“Š Marketing** | Customer Analytics | Churn prediction, lifetime value |
| **ğŸŒ Web** | Search Ranking | Search result ranking, ad placement |

## ğŸ“ Project Structure

```
Gradient Boosting/
â”œâ”€â”€ ğŸ““ GradientBoost+Classification+Implementation.ipynb    # Classification examples
â”œâ”€â”€ ğŸ““ Gradientboost+Regression+Implementation.ipynb       # Regression examples
â”œâ”€â”€ ğŸ“Š Travel.csv                                          # Travel dataset
â”œâ”€â”€ ğŸ“Š cardekho_imputated.csv                             # Car price dataset
â”œâ”€â”€ ğŸ–¼ï¸ auc.png                                             # AUC visualization
â””â”€â”€ ğŸ“„ README.md                                           # This documentation
```

## ğŸš€ Implementation Guide

### 1. Gradient Boosting Classification
```python
import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt

# Generate sample data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                          n_redundant=10, n_clusters_per_class=1, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(
    n_estimators=100,           # Number of boosting stages
    learning_rate=0.1,          # Learning rate shrinks contribution
    max_depth=3,                # Maximum depth of trees
    min_samples_split=2,        # Minimum samples to split node
    min_samples_leaf=1,         # Minimum samples in leaf
    subsample=1.0,              # Fraction of samples for fitting
    random_state=42
)

# Train the model
gb_classifier.fit(X_train, y_train)

# Make predictions
y_pred = gb_classifier.predict(X_test)
y_pred_proba = gb_classifier.predict_proba(X_test)

# Evaluate model
accuracy = accuracy_score(y_test, y_pred)
print(f"Gradient Boosting Accuracy: {accuracy:.4f}")

# Training progress
train_scores = gb_classifier.train_score_
test_scores = []

# Calculate test scores for each iteration
for i, pred in enumerate(gb_classifier.staged_predict_proba(X_test)):
    test_scores.append(accuracy_score(y_test, np.argmax(pred, axis=1)))

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(train_scores) + 1), train_scores, label='Training Score', color='blue')
plt.plot(range(1, len(test_scores) + 1), test_scores, label='Test Score', color='red')
plt.xlabel('Boosting Iterations')
plt.ylabel('Accuracy')
plt.title('Gradient Boosting Learning Curves')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 2. Gradient Boosting Regression
```python
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import make_regression

# Generate regression data
X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)

# Split the data
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

# Create Gradient Boosting regressor
gb_regressor = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    loss='squared_error',       # Loss function
    random_state=42
)

# Train the model
gb_regressor.fit(X_train_reg, y_train_reg)

# Make predictions
y_pred_reg = gb_regressor.predict(X_test_reg)

# Evaluate model
mse = mean_squared_error(y_test_reg, y_pred_reg)
r2 = r2_score(y_test_reg, y_pred_reg)

print(f"Gradient Boosting Regression Results:")
print(f"MSE: {mse:.4f}")
print(f"RÂ² Score: {r2:.4f}")

# Visualize predictions
plt.figure(figsize=(10, 6))
plt.scatter(y_test_reg, y_pred_reg, alpha=0.7)
plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('Gradient Boosting Regression: Predicted vs True Values')
plt.grid(True, alpha=0.3)
plt.show()
```

### 3. Feature Importance Analysis
```python
# Get feature importance
feature_importance = gb_classifier.feature_importances_
feature_names = [f'Feature_{i}' for i in range(X.shape[1])]

# Create importance dataframe
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(12, 8))
top_features = importance_df.head(15)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Feature Importance')
plt.title('Gradient Boosting Feature Importance (Top 15)')
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("Top 10 Most Important Features:")
print(importance_df.head(10))
```

### 4. Hyperparameter Tuning
```python
from sklearn.model_selection import RandomizedSearchCV

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'subsample': [0.8, 0.9, 1.0]
}

# Randomized search
random_search = RandomizedSearchCV(
    GradientBoostingClassifier(random_state=42),
    param_distributions=param_grid,
    n_iter=20,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)

print(f"Best parameters: {random_search.best_params_}")
print(f"Best cross-validation score: {random_search.best_score_:.4f}")

# Use best model
best_gb = random_search.best_estimator_
y_pred_best = best_gb.predict(X_test)
print(f"Test accuracy with best parameters: {accuracy_score(y_test, y_pred_best):.4f}")
```

### 5. Early Stopping Implementation
```python
# Gradient Boosting with early stopping
gb_early = GradientBoostingClassifier(
    n_estimators=1000,          # Large number for early stopping
    learning_rate=0.1,
    max_depth=3,
    validation_fraction=0.2,    # Fraction for validation
    n_iter_no_change=10,        # Early stopping rounds
    tol=1e-4,                   # Tolerance for early stopping
    random_state=42
)

# Train with early stopping
gb_early.fit(X_train, y_train)

print(f"Training stopped at iteration: {len(gb_early.train_score_)}")

# Plot training progress
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(gb_early.train_score_) + 1), gb_early.train_score_, 
         label='Training Score', color='blue')
plt.xlabel('Boosting Iterations')
plt.ylabel('Score')
plt.title('Gradient Boosting with Early Stopping')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Predictions with early stopping
y_pred_early = gb_early.predict(X_test)
print(f"Early stopping test accuracy: {accuracy_score(y_test, y_pred_early):.4f}")
```

## ğŸ“Š Advanced Techniques

### Stochastic Gradient Boosting
```python
# Compare different subsample ratios
subsample_values = [0.5, 0.7, 0.8, 1.0]
results = {}

for subsample in subsample_values:
    gb_stochastic = GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        subsample=subsample,
        random_state=42
    )
    
    gb_stochastic.fit(X_train, y_train)
    test_score = gb_stochastic.score(X_test, y_test)
    results[subsample] = test_score

# Visualize results
plt.figure(figsize=(10, 6))
subsample_vals = list(results.keys())
scores = list(results.values())
plt.plot(subsample_vals, scores, 'o-', color='blue')
plt.xlabel('Subsample Ratio')
plt.ylabel('Test Accuracy')
plt.title('Stochastic Gradient Boosting Performance')
plt.grid(True, alpha=0.3)
plt.show()

print("Stochastic Gradient Boosting Results:")
for subsample, score in results.items():
    print(f"Subsample {subsample}: {score:.4f}")
```

## ğŸ“š Learning Resources

- **Classification Notebook**: [`GradientBoost+Classification+Implementation.ipynb`](./GradientBoost+Classification+Implementation.ipynb)
- **Regression Notebook**: [`Gradientboost+Regression+Implementation.ipynb`](./Gradientboost+Regression+Implementation.ipynb)
- **Scikit-learn Documentation**: [Gradient Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)
- **Original Paper**: [Greedy Function Approximation: A Gradient Boosting Machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)

## ğŸ“ Key Takeaways

1. **ğŸ“ˆ Sequential Learning**: Each model corrects previous model's errors
2. **ğŸ¯ Loss Function Optimization**: Uses gradient descent in function space
3. **ğŸ”§ Hyperparameter Tuning**: Learning rate and tree depth are crucial
4. **âš¡ Early Stopping**: Prevents overfitting and saves computation
5. **ğŸŒ³ Feature Importance**: Provides insights into feature contributions
6. **ğŸ“Š Versatile**: Works for both classification and regression
7. **ğŸš€ Foundation for XGBoost**: Basis for more advanced boosting algorithms

---

*Navigate back to [Main Repository](../README.md) | Previous: [DBSCAN](../DBScan/README.md) | Next: [Hierarchical Clustering](../Hierarichal%20Clustering/README.md)*
